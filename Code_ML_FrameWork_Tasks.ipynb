{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code-ML-FrameWork-Tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "is7NrulvhGVU",
        "hwz5exOHhGMA",
        "jWqL939thF_I",
        "BwXHPXThhF6G",
        "MCibNQ17hFyo",
        "aOwU1HKKhFpN",
        "RcqYfk9QK_J8",
        "Dv9IjMoxjv5O",
        "HKSgLfiYjwJ3",
        "jCNvH8JBLj_C",
        "9N9mbGNpjwUB",
        "-7ectqJrjwbH",
        "cbsQt6xQnvMf",
        "GgOWZR5mh7xT",
        "0PTzV_xSnvPb",
        "sWhcJc82qb0R",
        "SL_wgAmenvS5",
        "WHrmiYyVnvlV",
        "Dx7kGQ2O6Uwf",
        "Gx8zp5IK6suW",
        "Xea6nqLl6sxN",
        "k3gVIFiJ6s0B",
        "Fb1rwqn96s_u",
        "SOrMVvoB9JW6",
        "nuHb6P_y9JdW",
        "HX7NdGy19Jgc",
        "o1gF0AeY9JkA",
        "LmkKBl429Jrc",
        "e5to9C-7-nFC",
        "TWM__0yD-nPT",
        "dDpsyJoV1_m8",
        "3jnzojqeEke6"
      ],
      "authorship_tag": "ABX9TyP6dFsZIeFkozR3z172FijW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IshantWadhwa4/MLFramework/blob/master/Code_ML_FrameWork_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ndvn7Y-hGgF",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Framework \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa1JCjW9hGan",
        "colab_type": "text"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1.   [Get Data](#Section1)\n",
        "2.   [Basic EDA](#Section2)\n",
        "3.   [Pre-Modeling](#Section3)\n",
        "4.   [Modeling](#Section4)\n",
        "5.   [Post Modeling](#Section5)\n",
        "6.   [ML Interpretation | Explaninable AI](#Section6)\n",
        "7.   [Model Deployment | MLOps](#Section7)\n",
        "8.   [Create Dashbord](#Section8) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is7NrulvhGVU",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section1\"></a>\n",
        "## 1. Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurboFF4hGQ3",
        "colab_type": "text"
      },
      "source": [
        "### Get Data from Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwz5exOHhGMA",
        "colab_type": "text"
      },
      "source": [
        "### Get Data from CSV/Text/Excel Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgWcWdFN8bJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data_path = 'https://storage.googleapis.com/industryanalytics/trans_fraud_data.csv'\n",
        "df = pd.read_csv(filepath_or_buffer = data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3QItnuzhGHt",
        "colab_type": "text"
      },
      "source": [
        "### Get Data from API's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdyTs6WUhGDb",
        "colab_type": "text"
      },
      "source": [
        "### Web scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWqL939thF_I",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section2\"></a>\n",
        "## 2. Basic EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwXHPXThhF6G",
        "colab_type": "text"
      },
      "source": [
        "### 1. Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMy_0jRi_gTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas_profiling==2.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_CIitCn_gZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas_profiling\n",
        "report = pandas_profiling.ProfileReport(df)\n",
        "#covert profile report as html file\n",
        "report.to_file(\"EDA.html\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCibNQ17hFyo",
        "colab_type": "text"
      },
      "source": [
        "### 2. DataFrame info,describe,head "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ybccO0IAJgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIZR7YAvAJoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info(verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43uQLc8WAJwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.options.display.float_format = \"{:.2f}\".format\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOwU1HKKhFpN",
        "colab_type": "text"
      },
      "source": [
        "### 3. Null and Zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbDPCVgxA49v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIb : need pandas lib for this function.\n",
        "# Parameters : Only required dataframe for which you want zeros and null for each column\n",
        "# return: dataframe with number of zeros and null\n",
        "\n",
        "def get_number_zeros_null(df):\n",
        "  '''\n",
        "       LIb : need pandas lib for this function.\n",
        "\n",
        "       Input : Only required dataframe for which you want zeros and null for each column       \n",
        "       Output: dataframe with number of zeros and null\n",
        "  '''\n",
        "  null_zero_dict={ }\n",
        "  null_zero_dict['Number_of_nulls'] = df.isnull().sum()\n",
        "  null_zero_dict['Number_of_zeros'] = (df==0).astype(int).sum()\n",
        "  return pd.DataFrame(null_zero_dict).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcqYfk9QK_J8",
        "colab_type": "text"
      },
      "source": [
        "### 4. Datatype of Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy86I_ppLFTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert type of columns\n",
        "\n",
        "def convert_type(df,list_column,list_type):\n",
        "  '''\n",
        "    convert column's data type\n",
        "    \n",
        "    Input: dataframe, list of columns you want to convert, type in which you want to convert\n",
        "    output: pandas data frame   \n",
        "  '''\n",
        "  for k,col in enumerate(list_column):\n",
        "    df[col] = df[col].astype(list_type[k])\n",
        "  return df\n",
        "\n",
        "\n",
        "## If you have dict we can use this\n",
        "dict_changes_type = zip(list_column,list_type)\n",
        "df.astype(dict_changes_type)\n",
        "\n",
        "\n",
        "#--------------- Convert String of date to pandas datetime format ------------------------------\n",
        "def convert_dateTime(df,column_list):\n",
        "  for col in column_list:\n",
        "    df[col] = pd.to_datetime(df[col])\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv9IjMoxjv5O",
        "colab_type": "text"
      },
      "source": [
        "### 5. Distribution of Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JztzCaQ5GLUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjAoMjgKGR-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_numeric_data_columns(df):\n",
        "  '''\n",
        "      return list of all numeric data columns name\n",
        "  '''\n",
        "  return list(df._get_numeric_data().columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JgPEIqlGp_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_catagorical_data_columns(df):\n",
        "  '''\n",
        "      return list of all catagoric data columns name\n",
        "  '''\n",
        "  return list(set(df.columns) - set(df._get_numeric_data().columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_17fwdfnjv8d",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1. Catagorical Data Distribution Single column\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCgMXdrxd7Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_countPlot_grid(df):\n",
        "  import math\n",
        "  fig=plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "  list_columns = get_catagorical_data_columns(df)\n",
        "  n_rows = math.ceil(len(list_columns)/3)\n",
        "  n_cols = 3\n",
        "  for i, var_name in enumerate(list_columns):\n",
        "    if len(df[var_name].unique()) < 8:\n",
        "      ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "      sns.countplot(x = var_name, data=df)\n",
        "      ax.set_title(var_name+\" Distribution\")\n",
        "  fig.tight_layout()  # Improves appearance a bit.\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH9tcvkqjv_i",
        "colab_type": "text"
      },
      "source": [
        "#### 4.2. Numerical Data Distribution Single column\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lunoSWtTd1kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "def draw_distributionPlot_grid(df):\n",
        "  import math\n",
        "  fig=plt.figure(num=None, figsize=(12, 50), dpi=80, facecolor='w', edgecolor='k')\n",
        "  list_columns = get_numeric_data_columns(df)\n",
        "  n_rows = math.ceil(len(list_columns)/3)\n",
        "  n_cols = 3\n",
        "  colors = []\n",
        "  for i in range(n_rows*n_cols):\n",
        "    colors.append('#%06X' % randint(0, 0xFFFFFF))\n",
        "  for i, var_name in enumerate(list_columns):\n",
        "    ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "    sns.distplot(df[var_name],hist=True,axlabel=var_name,color=colors[i])\n",
        "    ax.set_title(var_name+\" Distribution\")\n",
        "  fig.tight_layout()  # Improves appearance a bit.\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmqoh0bJ_Zj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how to see distribution of data if more than 5000 columns are there\n",
        "# Solution: One thing is find SD of all the columns and is SD is large than distribution is not good \n",
        "import numpy as np\n",
        "\n",
        "def get_SD_columns(df):\n",
        "  columns_numeric = get_numeric_data_columns(df)\n",
        "  dist_sd = {}\n",
        "  for col in columns_numeric:\n",
        "    dist_sd[col] = np.std(df[col])\n",
        "  result = pd.DataFrame(dist_sd,index=[0]).T\n",
        "  result['columns_name'] = result.index\n",
        "  result['SD'] = result[0]\n",
        "  result.reset_index(drop=True,inplace=True)\n",
        "  result.drop([0],axis=1,inplace=True)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0gvNF_1jwC2",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3. scatter plot relation b/w columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsAHR2w499y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "def plot_scatter_plotlyexpress(df,number_of_rows = 2000):\n",
        "  fig = px.scatter_matrix(df[:number_of_rows], dimensions= list(df.columns))\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7J-eTS_T3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "# class is any catagorical column name mainly target variable to see values\n",
        "def plot_scatter_class_plotlyexpress(df,class,number_of_rows = 2000):\n",
        "  fig = px.scatter_matrix(df[:number_of_rows], dimensions= list(df.columns),color = class)\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPJEXT-HjwF4",
        "colab_type": "text"
      },
      "source": [
        "#### 4.4. Heatmap for corelation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX_jvoEz-Qzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with all columns\n",
        "def heatmap_allcolumns(df):\n",
        "  sns.heatmap(df=df.corr(),annot=True, cmap=\"Blues\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxpGZaQoCHIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Heatmap for highly co-related(given threshold) columns\n",
        "# seaborn pandas and numpy\n",
        "\n",
        "def create_seaborn_heatmap_highcorelated(df,posThreshold,negThreshold):\n",
        "  '''\n",
        "      create Heatmap for highly co-related(given threshold) columns\n",
        "\n",
        "      Input: dataframe, positive threshold, negitive threshold\n",
        "      Plot: Heatmap\n",
        "  '''\n",
        "  df_corr = df.corr()\n",
        "  tempdf = df_corr[(df_corr > posThreshold) | (df_corr < -negThreshold)]\n",
        "  tempdf.replace(to_replace=1,value=np.nan,inplace=True)\n",
        "  tempdf.dropna(axis=1,how='all',inplace=True)\n",
        "  tempdf.dropna(axis=0,how='all',inplace=True)\n",
        "  sns.heatmap(tempdf,annot=True, cmap=\"Blues\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKSgLfiYjwJ3",
        "colab_type": "text"
      },
      "source": [
        "#### 4.5. Box plot for Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7x0LXzKKWCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_boxPlot_grid(df):\n",
        "  import math\n",
        "  fig=plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "  list_columns = get_numeric_data_columns(df)\n",
        "  n_rows = math.ceil(len(list_columns)/3)\n",
        "  n_cols = 3\n",
        "  for i, var_name in enumerate(list_columns):\n",
        "    ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "    sns.boxplot( y=df[var_name])\n",
        "    ax.set_title(var_name+\" Distribution\")\n",
        "  fig.tight_layout()  # Improves appearance a bit.\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNQ0qpsgi7tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## NOTE*** Before use this do dataframe scaling \n",
        "# If number of columns are very large we can use this.\n",
        "\n",
        "# The main motive is to find difference between max value and the quantile_threshold value and \n",
        "# draw a line graph to see is there any possible outlier \n",
        "# If SD=0 That mean all rows has same value in that column\n",
        "\n",
        "def get_thresholdDiff_outliers(df,quantile_threshold):\n",
        "  '''\n",
        "      The main motive is to find difference between max value and the quantile_threshold value and \n",
        "      draw a line graph to see is there any possible outlier. \n",
        "      \n",
        "      input: dataframe, quantile_threshold\n",
        "      output: df with column name and diff of max value and the quantile_threshold\n",
        "  '''\n",
        "  columns_numeric = get_numeric_data_columns(df)\n",
        "  quantile = []\n",
        "  dict_quantile={}\n",
        "  for col in columns_numeric:\n",
        "    dict_quantile[col] = df[col].max() - df[col].quantile(quantile_threshold)\n",
        "  result = pd.DataFrame(dict_quantile,index=[0]).T\n",
        "  result['columns_name'] = result.index\n",
        "  result['Difference'] = result[0]\n",
        "  result.reset_index(drop=True,inplace=True)\n",
        "  result.drop([0],axis=1,inplace=True)\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-MFaCOWjwQy",
        "colab_type": "text"
      },
      "source": [
        "### 6. Some questions you want answers from your data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNvH8JBLj_C",
        "colab_type": "text"
      },
      "source": [
        "### 7. EDA Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PhGtzzXLo6o",
        "colab_type": "text"
      },
      "source": [
        " Questions:\n",
        " 1. How many null values and zeros in the columns.\n",
        "      1. How i will fill na and zeros.(mean,median,mode,use simple models    and groupby )   \n",
        " 2. Distribution of data \n",
        "      1. Need upsampling(smote) or downsampling?\n",
        "      2. Need log trasformation for normal distribution?\n",
        "      3. Need scaling of data?\n",
        "      4. Need to solve Biasness in data?\n",
        " 3. Heat map\n",
        "      1. Corelation between columns(what columns are required)\n",
        "          Highly corelated columns can be removed\n",
        " 4. Outliers\n",
        "      1. Need to remove outliars or change its value to 75% etc  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N9mbGNpjwUB",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section3\"></a>\n",
        "## 3. Pre Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ectqJrjwbH",
        "colab_type": "text"
      },
      "source": [
        "### 1. Solve for null and zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g8ERimD9GXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_number_zeros_null(df)\n",
        "# 1. First see the percentage for null values, if it is more than 60% and no chance of geeting the values we can skip/remove that column\n",
        "# 2. See zeros has meaning in the data or they are just reprentation of null, if it is like null than replace all zero with null. (np.nan)\n",
        "# Solution\n",
        "# 1. Replace null with mean,median,mode (Chances of bisness of data)\n",
        "# 2. Groupby with some columns and calculate mean,median,mode and than replace with null\n",
        "# 3. Split null and not null, with not null create a model(classification/regration) and calculate for null values with that model\n",
        "# 4. KNN Imputer is a good way to fill NAN\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dU2l5FBus4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1\n",
        "\n",
        "# Remove column if it has more than threshold null values(threshold in percentage % )\n",
        "\n",
        "def remove_null_columns(df,threshold):\n",
        "  '''\n",
        "      Input: Dataframe,threshold\n",
        "      Output: list of columns to be drop\n",
        "  '''\n",
        "  null_values = get_number_zeros_null(df)\n",
        "  null_values.loc['null_percantage'] = (null_values.loc['Number_of_nulls']/df.shape[0])* 100\n",
        "  drop_column = []\n",
        "  for col in null_values.columns:\n",
        "    if null_values.loc['null_percantage',col] >= threshold:\n",
        "      drop_column.append(col)\n",
        "  return drop_column\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY1kK6wVFVdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 2\n",
        "\n",
        "# Replace null with mean median or mode, for numeric values median for catagorical value with mode\n",
        "\n",
        "def replace_null(df):\n",
        "  numeric_columns = get_numeric_data_columns(df)\n",
        "  catagoric_columns = get_catagorical_data_columns(df)\n",
        "  for col in numeric_columns:\n",
        "    df[col].fillna(df[col].median(),inplace=True)\n",
        "  for col in catagoric_columns:\n",
        "    df[col].fillna(df[col].mode()[0],inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def replace_null_columns(df,list_columns):\n",
        "  numeric_columns = get_numeric_data_columns(df)\n",
        "  catagoric_columns = get_catagorical_data_columns(df)\n",
        "  for col in list_columns:\n",
        "    if col in numeric_columns:\n",
        "      df[col].fillna(df[col].median(),inplace=True)\n",
        "    elif col in catagoric_columns:\n",
        "      df[col].fillna(df[col].mode()[0],inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbsQt6xQnvMf",
        "colab_type": "text"
      },
      "source": [
        "### 2. Solve for Outliars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk8-bTkniCBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First check the box plot and if outliars are meaningfull than do nothing. else you can do below tasks\n",
        "# Task 1: If we have very less outliars in a column like 1% than we can remove that row\n",
        "# Task 2: If ouliars are large you can replace tham with null and treat them as null values  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqVoWsj4H6ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Remove Rows of Outliers\n",
        "\n",
        "def remove_outlier_row(df,index_list):\n",
        "  df.drop(index_list,axis=0,inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgOWZR5mh7xT",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1 Solve for SD is zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Swxxt1iCmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if any column has zero SD that mean all the values are same so we can remove that column\n",
        "def remove_column_SD_Zero(df):\n",
        "  numeric_columns = get_numeric_data_columns(df)\n",
        "  drop_column = []\n",
        "  for col in numeric_columns:\n",
        "    if np.std(df[col])== 0 :\n",
        "      drop_column.append(col)\n",
        "  return drop_column\n",
        "\n",
        "df.drop(remove_column_SD_Zero(data),axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PTzV_xSnvPb",
        "colab_type": "text"
      },
      "source": [
        "### 3. Solve for Imbalance Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jyidLFwh-uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solution for disbalance data\n",
        "# 1. Upsampling for small class and downsampling fo big class\n",
        "# 2. For upsampling we can do (SMOTE) algo\n",
        "# 3. For downsampling we can dig deep in data and remove some high class rows (NearMiss)\n",
        "# READ : https://medium.com/@saeedAR/smote-and-near-miss-in-python-machine-learning-in-imbalanced-datasets-b7976d9a7a79"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVOKe9j9kfuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SMOTE will be implemented on scaler data\n",
        "# First do Encoding and scaling of Data\n",
        "\n",
        "# To handle class imbalance problem\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def do_smote_algo(X_ss,y,ratio):\n",
        "  '''\n",
        "      input:  X_ss standeredScale X value and y is taget variable column\n",
        "      output: X_new and y with upsampled data use for traing and testing data\n",
        "  '''\n",
        "  sm = SMOTE(random_state = 42, ratio = ratio) # here ratio tell the balance and imbalnace class number [We can do experiment on this]\n",
        "  X,y = sm.fit_sample(X_ss,y)\n",
        "  X_new = pd.DataFrame(data = X, columns = X_ss.columns)\n",
        "  return (X_new,y) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWhcJc82qb0R",
        "colab_type": "text"
      },
      "source": [
        "### 4. Binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9WrZJrHMEGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert continous data into classes we can use Bin\n",
        "\n",
        "def do_bining(df,column_name,number_bins,list_lables):\n",
        "  return pd.cut(df[column_name],bins=number_bins,labels= list_lables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL_wgAmenvS5",
        "colab_type": "text"
      },
      "source": [
        "### 5. Feature Engineering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHGMZRDnvVp",
        "colab_type": "text"
      },
      "source": [
        "#### 5.1. Grouping Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw_W-pyOnvYr",
        "colab_type": "text"
      },
      "source": [
        "#### 5.2. Feature Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1YXeC__nvba",
        "colab_type": "text"
      },
      "source": [
        "#### 5.3. Extracting Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k8YX_gKnveT",
        "colab_type": "text"
      },
      "source": [
        "#### Explore featuretools lib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0erI73xMnvhj",
        "colab_type": "text"
      },
      "source": [
        "### 6.Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqD9G-Al5nj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrmiYyVnvlV",
        "colab_type": "text"
      },
      "source": [
        "#### 6.1. Correlation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnvDu1wU5prZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If two columns are highly corelated to each other than we can simply remove those columns\n",
        "# use above function in Heatmap for its solution\n",
        "\n",
        "create_seaborn_heatmap_highcorelated(df,posThreshold,negThreshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv_6uutjnvpY",
        "colab_type": "text"
      },
      "source": [
        "#### 6.2. Chi-Squared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap5bm63G6UlT",
        "colab_type": "text"
      },
      "source": [
        "#### 6.3. Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvitzNpF6UsI",
        "colab_type": "text"
      },
      "source": [
        "#### 6.4. Lasso: SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx7kGQ2O6Uwf",
        "colab_type": "text"
      },
      "source": [
        "#### 6.5. Tree-based: SelectFromModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxIsjXBE6D0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Every Tree base algo give us feature importance we will use randomforest for this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTHvCqVyAwbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This give only list of important features\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def randomForestBased_FeatureImportance(X,y,max_features):\n",
        "  embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=max_features)\n",
        "  embeded_rf_selector.fit(X, y)\n",
        "  embeded_rf_support = embeded_rf_selector.get_support()\n",
        "  embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
        "  return embeded_rf_feature\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrWFyGFH6srZ",
        "colab_type": "text"
      },
      "source": [
        "#### We can use above all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx8zp5IK6suW",
        "colab_type": "text"
      },
      "source": [
        "### 7. Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6xSmapj3bls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://contrib.scikit-learn.org/categorical-encoding/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbW3Prb4al45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For nominal columns try OneHot, Hashing, LeaveOneOut, and Target encoding. Avoid OneHot for high cardinality columns and decision tree-based algorithms.\n",
        "# For ordinal columns try Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial are less likely to be helpful."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Qcvy4nVIIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Important point befor doing label encoding check the column in  odinal (label encoding) if Nominal (One hot encoding)\n",
        "\n",
        "def apply_encoding(df):\n",
        "  list_cata = get_catagorical_data_columns(df)\n",
        "  one_hot_list = []\n",
        "  label_list = []\n",
        "  for col in list_cata:\n",
        "    if len(df[col].unique()) >= 5:\n",
        "      label_list.append(col)\n",
        "    else:\n",
        "      one_hot_list.append(col)\n",
        "  df = do_onehot_Encoding(df,one_hot_list)\n",
        "  df = do_label_Encoding(df,label_list)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xea6nqLl6sxN",
        "colab_type": "text"
      },
      "source": [
        "#### 7.1 One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eERK8sfzjhVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# on-hot encoding and get_dummy\n",
        "\n",
        "import pandas as pd\n",
        "def do_onehot_Encoding(df,list_columns):\n",
        "  return pd.get_dummies(data = df, columns = list_columns)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3gVIFiJ6s0B",
        "colab_type": "text"
      },
      "source": [
        "#### 7.2 Label Encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiH-eWH6jyD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label Encoding is removing class to 0,1..\n",
        "\n",
        "# if column has more classes than we use lable encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def do_label_Encoding(df,column_list):\n",
        "  encode = LabelEncoder()\n",
        "  for col in column_list:\n",
        "    if isinstance(df[col].dtype, object):\n",
        "      df[col] = encode.fit_transform(df[col])\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taJv-O_qd6Ld",
        "colab_type": "text"
      },
      "source": [
        "#### 7.3 Binary encoding (For Odinal columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMfgni8Ad_07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Binary encoding converts a category into binary digits. Each binary digit creates one feature column. \n",
        "#If there are n unique categories, then binary encoding results in the only log(base 2)ⁿ features.\n",
        "# let we have 6 different class in a variable than if we do binary encoding[000,001,010,011,100,101,110] only 3 columns require to show this\n",
        "#but in one hot encoding we will have 6 different column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj9g1JpGe2A0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "def do_binary_encoding(df,list_columns):\n",
        "  for col in list_columns:\n",
        "    encoder = ce.BinaryEncoder(cols=[col])\n",
        "    dfbin = encoder.fit_transform(df[col])\n",
        "    df = pd.concat([df,dfbin],axis=1)\n",
        "  df.drop(list_column,axis=1,inplace=True)\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHxUtLKWrbAY",
        "colab_type": "text"
      },
      "source": [
        "#### 7.4 Frequency Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_GutZXCrkM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It is a way to utilize the frequency of the categories as labels. \n",
        "# In the cases where the frequency is related somewhat with the target variable, \n",
        "# it helps the model to understand and assign the weight in direct and inverse proportion, depending on the nature of the data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIsVqBB8rkRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frequancy_encoding(df,cat_column):\n",
        "  fe = df.groupby(cat_column).size()/len(df)\n",
        "  df.loc[:,cat_column+'_FE'] = df[cat_column].map(fe)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hoi57Q2zbBa",
        "colab_type": "text"
      },
      "source": [
        "#### 7.5 Mean Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yPx4TNxze4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mean Encoding or Target Encoding is one viral encoding approach followed by Kagglers.\n",
        "# There are many variations of this. Here I will cover the basic version and smoothing version. \n",
        "# Mean encoding is similar to label encoding, except here labels are correlated directly with the target. \n",
        "#For example, in mean target encoding for each category in the feature label is decided with the mean value of the target variable on a training data. \n",
        "#This encoding method brings out the relation between similar categories, but the connections are bounded within the categories and target itself. \n",
        "#1.  Select a categorical variable you would like to transform\n",
        "#2. Group by the categorical variable and obtain aggregated sum over the “Target” variable. (total number of 1’s for each category in ‘Temperature’)\n",
        "#3. Group by the categorical variable and obtain aggregated count over “Target” variable\n",
        "#4. Divide the step 2 / step 3 results and join it back with the train."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-RinShKz2_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_encoding(df,colum_name,target_name):\n",
        "  mean_encode = df.groupby(column_name)[target_name].mean()\n",
        "  df.loc[:,colum_name+'_ME'] = df[colum_name].map(mean_encode)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU2VugiA6s25",
        "colab_type": "text"
      },
      "source": [
        "#### 7.6 Helmert Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7CgOLwb6s5q",
        "colab_type": "text"
      },
      "source": [
        "#### 7.7 Hashing Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FojjFMX6s8y",
        "colab_type": "text"
      },
      "source": [
        "#### 7.8 M-estimator Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb1rwqn96s_u",
        "colab_type": "text"
      },
      "source": [
        "### 8. Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnIFm-KG6tDo",
        "colab_type": "text"
      },
      "source": [
        "#### 8.1. Principal Component Analysis(PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwIVz56zrFCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will do dimred using PCA with sklearn\n",
        "\n",
        "# Always use scale data for improving performance\n",
        "# Use X for dimensionality reduction \n",
        "# This will do the fast computation and not much change in the accuracy\n",
        "# we dont know the columns made by PCA\n",
        "\n",
        "def dimred_PCA(X,information_loss):\n",
        "  '''\n",
        "    input: information_loss how much info loss is good for you in percentage, X is the dependent variables/columns dataframe\n",
        "\n",
        "  '''\n",
        "  from sklearn.decomposition import PCA\n",
        "  info = 1 - (infomation_loss/100)\n",
        "  pca = PCA(info).fit(X)\n",
        "  Variance_Explained_PCA_graph(pca)\n",
        "  print('number of columns left are: {}'.format(pca.n_components_))\n",
        "  transform_df = pca.transform(X)\n",
        "  return transform_df\n",
        "\n",
        "\n",
        "def Variance_Explained_PCA_graph (pca):\n",
        "  var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
        "  plt.ylabel('% Variance Explained')\n",
        "  plt.xlabel('Number of Features')\n",
        "  plt.title('PCA Analysis')\n",
        "  plt.ylim(30,100.5)\n",
        "  plt.style.context('seaborn-whitegrid')\n",
        "  plt.plot(var)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGIZl456tQW",
        "colab_type": "text"
      },
      "source": [
        "#### 8.2 Linear Discriminant Analysis (LDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOrMVvoB9JW6",
        "colab_type": "text"
      },
      "source": [
        "### 9. Scaling of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OSaCaXZWKNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scaling of the value X\n",
        "# StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def do_standardScale(X):\n",
        "  sc = StandardScaler()\n",
        "  sc.fit(X)\n",
        "  x_sc = sc.transform(X)\n",
        "  x_sc_df = pd.DataFrame(x_sc,columns=X.columns)\n",
        "  return x_sc_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpW_Lyw9JZ-",
        "colab_type": "text"
      },
      "source": [
        "### 10. Log Trasformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHb6P_y9JdW",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section4\"></a>\n",
        "## 4. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX7NdGy19Jgc",
        "colab_type": "text"
      },
      "source": [
        "### 1. Data Spliting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZGR3fkyUEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normal Split\n",
        "# Spliting of data\n",
        "X = df.drop('target',axis = 1)\n",
        "y= df['target']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34o_2xeViqMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle split is use as CV in cross validation and Grid search same as train test split \n",
        "\n",
        "# n_split is number of split and test_size is how much % of rows you want in test data set\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def do_shufflesplit(n_splits,test_size):  \n",
        "  return ShuffleSplit(n_splits = n_splits, test_size = test_size/100, train_size = 1-(test_size/100), random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1gF0AeY9JkA",
        "colab_type": "text"
      },
      "source": [
        "### 2. Cross Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0UAk5LimY4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross validation with SKlearn\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "def do_crossValidation(algo_object,X,y,cv,scoring):\n",
        "  '''\n",
        "      Return : return data frame \n",
        "  '''\n",
        "  algo_name = algo_object.__class__.__name__ \n",
        "  cc = cross_validate(algo_object, X, y, cv=cv, return_train_score=True, return_estimator=True, n_jobs=-1, scoring=scoring)\n",
        "  return get_crossValidation_Result(algo_name,cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Poo1wzo9_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_crossValidation_Result(algo_name,result):\n",
        "  result_dict = {}\n",
        "  result_dict['Algo Name'] = algo_name\n",
        "  result_dict['Time'] = result['fit_time'].mean()\n",
        "  result_dict['Algo Train Accuracy Score']= result['train_score'].mean()\n",
        "  result_dict['Algo Test Accuracy Score'] = result['test_score'].mean()\n",
        "  result_dict['Algo Test Accuracy 3*STD'] = result['test_score'].std()*3\n",
        "  return pd.DataFrame(result_dict) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlK7S0RD8MlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How to use it\n",
        "# Sample \n",
        "models_Classification = {\n",
        "    \n",
        "    'LogisticRegression':LogisticRegression(),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
        "    'RandomForestClassifier': RandomForestClassifier()\n",
        "    \n",
        "}\n",
        "array_result = []\n",
        "for count,(key,value) in enumerate(models_Classification.items()):\n",
        "  result = do_crossValidation(value,X,y,cv=2,scoring='f1')\n",
        "  print(result)\n",
        "  array_result.append(result)\n",
        "df_result = pd.DataFrame(array_result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQKvm5bv9Jnf",
        "colab_type": "text"
      },
      "source": [
        "### 3. Parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBfqY0ze4wPv",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n0GaROMgbZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lavi Code for GridSearch\n",
        "\n",
        "# Helper Class for Initilizing GridSearch\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "class EstimatorSelectionHelper:\n",
        "\n",
        "    # Init function as we create object of this class this function call\n",
        "    def __init__(self, models, params):\n",
        "      '''\n",
        "      models and params are dict\n",
        "      '''\n",
        "      if not set(models.keys()).issubset(set(params.keys())):\n",
        "        missing_params = list(set(models.keys()) - set(params.keys()))\n",
        "        raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
        "      self.models = models\n",
        "      self.params = params\n",
        "      self.keys = models.keys()\n",
        "      self.grid_searches = {}\n",
        "      self.best_params = {}\n",
        "\n",
        "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=True):\n",
        "        for key in self.keys:\n",
        "            print(\"Running GridSearchCV for %s.\" % key)\n",
        "            model = self.models[key]\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, refit=refit,\n",
        "                              return_train_score=True)\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs  \n",
        "            self.best_params[key]  = str(gs.best_params_)\n",
        "\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            print(k)\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params,all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFEE57zx42OT",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 HyperOpt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr3v7Tjagbkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkKBl429Jrc",
        "colab_type": "text"
      },
      "source": [
        "### 4. Model *Libs*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt9YGut64a5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWIK9SOGYJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34RBvLz9JxZ",
        "colab_type": "text"
      },
      "source": [
        "### 5. List of Model and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhyscef6G7qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models_Classification = {\n",
        "    \n",
        "    'LogisticRegression':LogisticRegression(),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
        "    'RandomForestClassifier': RandomForestClassifier()\n",
        "    \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiJauOqDQITm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models_Regression = {\n",
        "    \n",
        "    'LinearRegression':LinearRegression(),\n",
        "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
        "    'RandomForestRegressor': RandomForestRegressor()\n",
        "    \n",
        "}\n",
        "\n",
        "params_Regression = {\n",
        "    'LinearRegression': { 'fit_intercept':[True,False]},  # l1 lasso l2 ridge\n",
        "    'DecisionTreeRegressor': {'criterion' : ['mse', 'friedman_mse', 'mae'], 'splitter' : ['random', 'best'], 'max_depth':[2,5,10], 'min_samples_leaf':[2,5,10]},\n",
        "    'RandomForestRegressor': { 'n_estimators': [16, 32] }    \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag1b7XzJ-nCF",
        "colab_type": "text"
      },
      "source": [
        "### 6. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5to9C-7-nFC",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section5\"></a>\n",
        "## 5. Post Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBY-o1ox-nLQ",
        "colab_type": "text"
      },
      "source": [
        "### 1. Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWM__0yD-nPT",
        "colab_type": "text"
      },
      "source": [
        "### 2. Confusion Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T7f1W_iWhN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.accuracy_score(y_test,lr_pred))\n",
        "\n",
        "print('***************************************')\n",
        "print('Confusion matrix')\n",
        "dt_cfm=metrics.confusion_matrix(y_test, lr_pred)\n",
        "\n",
        "\n",
        "lbl1=[\"Predicted 1\", \"Predicted 2\"]\n",
        "lbl2=[\"Actual 1\", \"Actual 2\"]\n",
        "\n",
        "sns.heatmap(dt_cfm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=lbl1, yticklabels=lbl2)\n",
        "plt.show()\n",
        "\n",
        "print('****************************************')\n",
        "print(metrics.classification_report(y_test,lr_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QjjIqTn-nSx",
        "colab_type": "text"
      },
      "source": [
        "### 3. Precession, Recall, F1, AUC ROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcpQ_wS7-nV_",
        "colab_type": "text"
      },
      "source": [
        "### 4. RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDpsyJoV1_m8",
        "colab_type": "text"
      },
      "source": [
        "## DataScience Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG00hUzz2QM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assumtions\n",
        "# 1. Working on supervise learning with data in csv format "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFg56QPd2j3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lib\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcUsmqYP2ELX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Start(dataPath,tagret_column):\n",
        "  df = pd.read_csv(filepath_or_buffer = dataPath)\n",
        "  # Basic EDA\n",
        "  print(\"Shape of Data\",df.shape)\n",
        "  # Number of zeros and null in the data\n",
        "  zero_null = get_number_zeros_null(df)\n",
        "  print(zero_null)\n",
        "  # info or datatype of columns\n",
        "  print(df.info())\n",
        "  # Distribution of Data\n",
        "  #  1. Catagoricl plot\n",
        "  draw_countPlot_grid(df)\n",
        "  #  2. Numeric Plot\n",
        "  draw_distributionPlot_grid(df)\n",
        "  #   3. Scatter plot\n",
        "  plot_scatter_plotlyexpress(df)\n",
        "  #   4. Heat Map\n",
        "  heatmap_allcolumns(df)\n",
        "  #   5. Highly corelated heatmap\n",
        "  a = input(\"Do you want to see highly corelated columns heatmap press 1\")\n",
        "  if a == '1':\n",
        "    posThreshold = input(\"Please Enter max positve corelation Threshold Example 0.9 \")\n",
        "    negThreshold = input(\"Please Enter max negitive corelation Threshold Example 0.9\")\n",
        "    create_seaborn_heatmap_highcorelated(df,float(posThreshold),float(negThreshold))\n",
        "  # Solve for Null Values\n",
        "  df.drop(remove_null_columns(df,60),axis=1,inplace=True)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jnzojqeEke6",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section6\"></a>\n",
        "## 6. ML Interpretation | Explaninable AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDCd2pWE_wji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIME and SHAP lib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtt-0T6MAMZh",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section7\"></a>\n",
        "## 7. Model Deployment | MLOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPl3IGEBAMeM",
        "colab_type": "text"
      },
      "source": [
        "<a id = \"Section8\"></a>\n",
        "## 8. Create Dashbord"
      ]
    }
  ]
}