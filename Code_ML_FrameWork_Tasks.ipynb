{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code-ML-FrameWork-Tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "is7NrulvhGVU",
        "hwz5exOHhGMA",
        "BwXHPXThhF6G",
        "MCibNQ17hFyo",
        "aOwU1HKKhFpN",
        "RcqYfk9QK_J8",
        "Dv9IjMoxjv5O",
        "_17fwdfnjv8d",
        "VH9tcvkqjv_i",
        "k0gvNF_1jwC2",
        "lPJEXT-HjwF4",
        "HKSgLfiYjwJ3",
        "jCNvH8JBLj_C",
        "GgOWZR5mh7xT",
        "SL_wgAmenvS5",
        "0erI73xMnvhj",
        "Gx8zp5IK6suW",
        "Fb1rwqn96s_u",
        "wnIFm-KG6tDo",
        "SOrMVvoB9JW6",
        "e5to9C-7-nFC"
      ],
      "authorship_tag": "ABX9TyPWEGSaHV+hLJzJnBALB48v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IshantWadhwa4/MLFramework/blob/master/Code_ML_FrameWork_Tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ndvn7Y-hGgF",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Framework \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa1JCjW9hGan",
        "colab_type": "text"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1.   [Get Data](#Section1)\n",
        "2.   [Basic EDA](#Section2)\n",
        "3.   [Pre-Modeling](#Section3)\n",
        "4.   [Modeling](#Section4)\n",
        "5.   [Post Modeling](#Section5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is7NrulvhGVU",
        "colab_type": "text"
      },
      "source": [
        "<a id = Section1></a>\n",
        "## 1. Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurboFF4hGQ3",
        "colab_type": "text"
      },
      "source": [
        "### Get Data from Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwz5exOHhGMA",
        "colab_type": "text"
      },
      "source": [
        "### Get Data from CSV/Text/Excel Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgWcWdFN8bJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data_path = 'https://storage.googleapis.com/industryanalytics/trans_fraud_data.csv'\n",
        "df = pd.read_csv(filepath_or_buffer = data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3QItnuzhGHt",
        "colab_type": "text"
      },
      "source": [
        "### Get Data from API's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdyTs6WUhGDb",
        "colab_type": "text"
      },
      "source": [
        "### Web scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWqL939thF_I",
        "colab_type": "text"
      },
      "source": [
        "<a id = Section2></a>\n",
        "## 2. Basic EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwXHPXThhF6G",
        "colab_type": "text"
      },
      "source": [
        "### 1. Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMy_0jRi_gTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas_profiling==2.5.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_CIitCn_gZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas_profiling\n",
        "report = pandas_profiling.ProfileReport(df)\n",
        "#covert profile report as html file\n",
        "report.to_file(\"EDA.html\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCibNQ17hFyo",
        "colab_type": "text"
      },
      "source": [
        "### 2. DataFrame info,describe,head "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ybccO0IAJgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIZR7YAvAJoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.info(verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43uQLc8WAJwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pd.options.display.float_format = \"{:.2f}\".format\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOwU1HKKhFpN",
        "colab_type": "text"
      },
      "source": [
        "### 3. Null and Zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbDPCVgxA49v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LIb : need pandas lib for this function.\n",
        "# Parameters : Only required dataframe for which you want zeros and null for each column\n",
        "# return: dataframe with number of zeros and null\n",
        "\n",
        "def get_number_zeros_null(df):\n",
        "  '''\n",
        "       LIb : need pandas lib for this function.\n",
        "\n",
        "       Input : Only required dataframe for which you want zeros and null for each column       \n",
        "       Output: dataframe with number of zeros and null\n",
        "  '''\n",
        "  null_zero_dict={ }\n",
        "  null_zero_dict['Number_of_nulls'] = df.isnull().sum()\n",
        "  null_zero_dict['Number_of_zeros'] = (df==0).astype(int).sum()\n",
        "  return pd.DataFrame(null_zero_dict).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcqYfk9QK_J8",
        "colab_type": "text"
      },
      "source": [
        "### 4. Datatype of Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy86I_ppLFTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert type of columns\n",
        "\n",
        "def convert_type(df,list_column,list_type):\n",
        "  '''\n",
        "    convert column's data type\n",
        "    \n",
        "    Input: dataframe, list of columns you want to convert, type in which you want to convert\n",
        "    output: pandas data frame   \n",
        "  '''\n",
        "  for k,col in enumerate(list_column):\n",
        "    df[col] = df[col].astype(list_type[k])\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv9IjMoxjv5O",
        "colab_type": "text"
      },
      "source": [
        "### 5. Distribution of Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JztzCaQ5GLUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjAoMjgKGR-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_numeric_data_columns(df):\n",
        "  '''\n",
        "      return list of all numeric data columns name\n",
        "  '''\n",
        "  return list(df._get_numeric_data().columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JgPEIqlGp_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_catagorical_data_columns(df):\n",
        "  '''\n",
        "      return list of all catagoric data columns name\n",
        "  '''\n",
        "  return list(set(df.columns) - set(df._get_numeric_data().columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_17fwdfnjv8d",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1. Catagorical Data Distribution Single column\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCgMXdrxd7Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_countPlot_grid(df):\n",
        "  import math\n",
        "  fig=plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "  list_columns = get_catagorical_data_columns(df)\n",
        "  n_rows = math.ceil(len(list_columns)/3)\n",
        "  n_cols = 3\n",
        "  for i, var_name in enumerate(list_columns):\n",
        "    if len(df[var_name].unique()) < 8:\n",
        "      ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "      sns.countplot(x = var_name, data=df)\n",
        "      ax.set_title(var_name+\" Distribution\")\n",
        "  fig.tight_layout()  # Improves appearance a bit.\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH9tcvkqjv_i",
        "colab_type": "text"
      },
      "source": [
        "#### 4.2. Numerical Data Distribution Single column\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lunoSWtTd1kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_distributionPlot_grid(df):\n",
        "  import math\n",
        "  fig=plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "  list_columns = get_numeric_data_columns(df)\n",
        "  n_rows = math.ceil(len(list_columns)/3)\n",
        "  n_cols = 3\n",
        "  for i, var_name in enumerate(list_columns):\n",
        "    ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "    sns.distplot(df[var_name],hist=True,axlabel=var_name)\n",
        "    ax.set_title(var_name+\" Distribution\")\n",
        "  fig.tight_layout()  # Improves appearance a bit.\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmqoh0bJ_Zj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how to see distribution of data if more than 5000 columns are there\n",
        "# Solution: One thing is find SD of all the columns and is SD is large than distribution is not good \n",
        "import numpy as np\n",
        "\n",
        "def get_SD_columns(df):\n",
        "  columns_numeric = get_numeric_data_columns(df)\n",
        "  dist_sd = {}\n",
        "  for col in columns_numeric:\n",
        "    dist_sd[col] = np.std(df[col])\n",
        "  result = pd.DataFrame(dist_sd,index=[0]).T\n",
        "  result['columns_name'] = result.index\n",
        "  result['SD'] = result[0]\n",
        "  result.reset_index(drop=True,inplace=True)\n",
        "  result.drop([0],axis=1,inplace=True)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0gvNF_1jwC2",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3. scatter plot relation b/w columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsAHR2w499y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "def plot_scatter_plotlyexpress(df,number_of_rows = 2000):\n",
        "  fig = px.scatter_matrix(df[:number_of_rows], dimensions= list(df.columns))\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7J-eTS_T3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "# class is any catagorical column name mainly target variable to see values\n",
        "def plot_scatter_class_plotlyexpress(df,class,number_of_rows = 2000):\n",
        "  fig = px.scatter_matrix(df[:number_of_rows], dimensions= list(df.columns),color = class)\n",
        "  fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPJEXT-HjwF4",
        "colab_type": "text"
      },
      "source": [
        "#### 4.4. Heatmap for corelation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX_jvoEz-Qzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with all columns\n",
        "def heatmap_allcolumns(df):\n",
        "  sns.heatmap(df=df.corr(),annot=True, cmap=\"Blues\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxpGZaQoCHIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Heatmap for highly co-related(given threshold) columns\n",
        "# seaborn pandas and numpy\n",
        "\n",
        "def create_seaborn_heatmap_highcorelated(df,posThreshold,negThreshold):\n",
        "  '''\n",
        "      create Heatmap for highly co-related(given threshold) columns\n",
        "\n",
        "      Input: dataframe, positive threshold, negitive threshold\n",
        "      Plot: Heatmap\n",
        "  '''\n",
        "  df_corr = df.corr()\n",
        "  tempdf = df_corr[(df_corr > posThreshold) | (df_corr < -negThreshold)]\n",
        "  tempdf.replace(to_replace=1,value=np.nan,inplace=True)\n",
        "  tempdf.dropna(axis=1,how='all',inplace=True)\n",
        "  tempdf.dropna(axis=0,how='all',inplace=True)\n",
        "  sns.heatmap(tempdf,annot=True, cmap=\"Blues\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKSgLfiYjwJ3",
        "colab_type": "text"
      },
      "source": [
        "#### 4.5. Box plot for Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7x0LXzKKWCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_boxPlot_grid(df):\n",
        "  import math\n",
        "  fig=plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "  list_columns = get_numeric_data_columns(df)\n",
        "  n_rows = math.ceil(len(list_columns)/3)\n",
        "  n_cols = 3\n",
        "  for i, var_name in enumerate(list_columns):\n",
        "    ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "    sns.boxplot( y=df[var_name])\n",
        "    ax.set_title(var_name+\" Distribution\")\n",
        "  fig.tight_layout()  # Improves appearance a bit.\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNQ0qpsgi7tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## NOTE*** Before use this do dataframe scaling \n",
        "# If number of columns are very large we can use this.\n",
        "\n",
        "# The main motive is to find difference between max value and the quantile_threshold value and \n",
        "# draw a line graph to see is there any possible outlier \n",
        "# If SD=0 That mean all rows has same value in that column\n",
        "\n",
        "def get_thresholdDiff_outliers(df,quantile_threshold):\n",
        "  '''\n",
        "      The main motive is to find difference between max value and the quantile_threshold value and \n",
        "      draw a line graph to see is there any possible outlier. \n",
        "      \n",
        "      input: dataframe, quantile_threshold\n",
        "      output: df with column name and diff of max value and the quantile_threshold\n",
        "  '''\n",
        "  columns_numeric = get_numeric_data_columns(df)\n",
        "  quantile = []\n",
        "  dict_quantile={}\n",
        "  for col in columns_numeric:\n",
        "    dict_quantile[col] = df[col].max() - df[col].quantile(quantile_threshold)\n",
        "  result = pd.DataFrame(dict_quantile,index=[0]).T\n",
        "  result['columns_name'] = result.index\n",
        "  result['Difference'] = result[0]\n",
        "  result.reset_index(drop=True,inplace=True)\n",
        "  result.drop([0],axis=1,inplace=True)\n",
        "  return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-MFaCOWjwQy",
        "colab_type": "text"
      },
      "source": [
        "### 6. Some questions you want answers from your data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCNvH8JBLj_C",
        "colab_type": "text"
      },
      "source": [
        "### 7. EDA Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PhGtzzXLo6o",
        "colab_type": "text"
      },
      "source": [
        " Questions:\n",
        " 1. How many null values and zeros in the columns.\n",
        "      1. How i will fill na and zeros.(mean,median,mode,use simple models    and groupby )   \n",
        " 2. Distribution of data \n",
        "      1. Need upsampling(smote) or downsampling?\n",
        "      2. Need log trasformation for normal distribution?\n",
        "      3. Need scaling of data?\n",
        "      4. Need to solve Biasness in data?\n",
        " 3. Heat map\n",
        "      1. Corelation between columns(what columns are required)\n",
        "          Highly corelated columns can be removed\n",
        " 4. Outliers\n",
        "      1. Need to remove outliars or change its value to 75% etc  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N9mbGNpjwUB",
        "colab_type": "text"
      },
      "source": [
        "<a id = Section3></a>\n",
        "## 3. Pre Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ectqJrjwbH",
        "colab_type": "text"
      },
      "source": [
        "### 1. Solve for null and zero values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g8ERimD9GXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_number_zeros_null(df)\n",
        "# 1. First see the percentage for null values, if it is more than 60% and no chance of geeting the values we can skip/remove that column\n",
        "# 2. See zeros has meaning in the data or they are just reprentation of null, if it is like null than replace all zero with null. (np.nan)\n",
        "# Solution\n",
        "# 1. Replace null with mean,median,mode (Chances of bisness of data)\n",
        "# 2. Groupby with some columns and calculate mean,median,mode and than replace with null\n",
        "# 3. Split null and not null, with not null create a model(classification/regration) and calculate for null values with that model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dU2l5FBus4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 1\n",
        "\n",
        "# Remove column if it has more than threshold null values(threshold in percentage % )\n",
        "\n",
        "def remove_null_columns(df,threshold):\n",
        "  '''\n",
        "      Input: Dataframe,threshold\n",
        "      Output: list of columns to be drop\n",
        "  '''\n",
        "  null_values = get_number_zeros_null(df)\n",
        "  null_values.loc['null_percantage'] = (null_values.loc['Number_of_nulls']/df.shape[0])* 100\n",
        "  drop_column = []\n",
        "  for col in null_values.columns:\n",
        "    if null_values.loc['null_percantage',col] >= threshold:\n",
        "      drop_column.append(col)\n",
        "  return drop_column\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY1kK6wVFVdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task 2\n",
        "\n",
        "# Replace null with mean median or mode, for numeric values median for catagorical value with mode\n",
        "\n",
        "def replace_null(df):\n",
        "  numeric_columns = get_numeric_data_columns(df)\n",
        "  catagoric_columns = get_catagorical_data_columns(df)\n",
        "  for col in numeric_columns:\n",
        "    df[col].fillna(df[col].median(),inplace=True)\n",
        "  for col in catagoric_columns:\n",
        "    df[col].fillna(df[col].mode(),inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def replace_null_columns(df,list_columns):\n",
        "  numeric_columns = get_numeric_data_columns(df)\n",
        "  catagoric_columns = get_catagorical_data_columns(df)\n",
        "  for col in list_columns:\n",
        "    if col in numeric_columns:\n",
        "      df[col].fillna(df[col].median(),inplace=True)\n",
        "    elif col in catagoric_columns:\n",
        "      f[col].fillna(df[col].mode(),inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbsQt6xQnvMf",
        "colab_type": "text"
      },
      "source": [
        "### 2. Solve for Outliars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk8-bTkniCBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First check the box plot and if outliars are meaningfull than do nothing. else you can do below tasks\n",
        "# Task 1: If we have very less outliars in a column like 1% than we can remove that row\n",
        "# Task 2: If ouliars are large you can replace tham with null and treat them as null values  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgOWZR5mh7xT",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1 Solve for SD is zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Swxxt1iCmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if any column has zero SD that mean all the values are same so we can remove that column\n",
        "def remove_column_SD_Zero(df):\n",
        "  numeric_columns = get_numeric_data_columns(df)\n",
        "  drop_column = []\n",
        "  for col in numeric_columns:\n",
        "    if np.std(df[col])== 0 :\n",
        "      drop_column.append(col)\n",
        "  return drop_column\n",
        "\n",
        "df.drop(remove_column_SD_Zero(data),axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PTzV_xSnvPb",
        "colab_type": "text"
      },
      "source": [
        "### 3. Solve for Dis-balalance Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWhcJc82qb0R",
        "colab_type": "text"
      },
      "source": [
        "### 4. Binning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL_wgAmenvS5",
        "colab_type": "text"
      },
      "source": [
        "### 5. Feature Engineering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHGMZRDnvVp",
        "colab_type": "text"
      },
      "source": [
        "#### 5.1. Grouping Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw_W-pyOnvYr",
        "colab_type": "text"
      },
      "source": [
        "#### 5.2. Feature Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1YXeC__nvba",
        "colab_type": "text"
      },
      "source": [
        "#### 5.3. Extracting Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k8YX_gKnveT",
        "colab_type": "text"
      },
      "source": [
        "#### Explore featuretools lib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0erI73xMnvhj",
        "colab_type": "text"
      },
      "source": [
        "### 6.Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrmiYyVnvlV",
        "colab_type": "text"
      },
      "source": [
        "#### 6.1. Correlation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv_6uutjnvpY",
        "colab_type": "text"
      },
      "source": [
        "#### 6.2. Chi-Squared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap5bm63G6UlT",
        "colab_type": "text"
      },
      "source": [
        "#### 6.3. Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvitzNpF6UsI",
        "colab_type": "text"
      },
      "source": [
        "#### 6.4. Lasso: SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx7kGQ2O6Uwf",
        "colab_type": "text"
      },
      "source": [
        "#### 6.5. Tree-based: SelectFromModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrWFyGFH6srZ",
        "colab_type": "text"
      },
      "source": [
        "#### We can use above all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx8zp5IK6suW",
        "colab_type": "text"
      },
      "source": [
        "### 7. Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xea6nqLl6sxN",
        "colab_type": "text"
      },
      "source": [
        "#### 7.1 One-Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3gVIFiJ6s0B",
        "colab_type": "text"
      },
      "source": [
        "#### 7.2 Label Encoding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU2VugiA6s25",
        "colab_type": "text"
      },
      "source": [
        "#### 7.3 Helmert Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7CgOLwb6s5q",
        "colab_type": "text"
      },
      "source": [
        "#### 7.4 Hashing Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FojjFMX6s8y",
        "colab_type": "text"
      },
      "source": [
        "#### 7.5 M-estimator Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb1rwqn96s_u",
        "colab_type": "text"
      },
      "source": [
        "### 8. Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnIFm-KG6tDo",
        "colab_type": "text"
      },
      "source": [
        "#### 8.1. Principal Component Analysis(PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwIVz56zrFCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will do dimred using PCA with sklearn\n",
        "\n",
        "# Always use scale data for improving performance\n",
        "# Use X for dimensionality reduction \n",
        "# This will do the fast computation and not much change in the accuracy\n",
        "# we dont know the columns made by PCA\n",
        "\n",
        "def dimred_PCA(X,information_loss):\n",
        "  '''\n",
        "    input: information_loss how much info loss is good for you in percentage, X is the dependent variables/columns dataframe\n",
        "\n",
        "  '''\n",
        "  from sklearn.decomposition import PCA\n",
        "  info = 1 - (infomation_loss/100)\n",
        "  pca = PCA(info).fit(X)\n",
        "  #Variance_Explained_PCA_graph (pca)\n",
        "  print('number of columns left are: {}'.format(pca.n_components_))\n",
        "  transform_df = pca.transform(X)\n",
        "  return transform_df\n",
        "\n",
        "\n",
        "def Variance_Explained_PCA_graph (pca):\n",
        "  var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
        "  plt.ylabel('% Variance Explained')\n",
        "  plt.xlabel('Number of Features')\n",
        "  plt.title('PCA Analysis')\n",
        "  plt.ylim(30,100.5)\n",
        "  plt.style.context('seaborn-whitegrid')\n",
        "  plt.plot(var)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGIZl456tQW",
        "colab_type": "text"
      },
      "source": [
        "#### 8.2 Linear Discriminant Analysis (LDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOrMVvoB9JW6",
        "colab_type": "text"
      },
      "source": [
        "### 9. Scaling of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OSaCaXZWKNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scaling of the value X\n",
        "# StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(X)\n",
        "x_sc = sc.transform(X)\n",
        "x_sc_df = pd.DataFrame(x_sc,columns=X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpW_Lyw9JZ-",
        "colab_type": "text"
      },
      "source": [
        "### 10. Log Trasformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHb6P_y9JdW",
        "colab_type": "text"
      },
      "source": [
        "<a id = Section4></a>\n",
        "## 4. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX7NdGy19Jgc",
        "colab_type": "text"
      },
      "source": [
        "### 1. Data Spliting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZGR3fkyUEzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normal Split\n",
        "# Spliting of data\n",
        "X = df.drop('target',axis = 1)\n",
        "y= df['target']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34o_2xeViqMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle split is use as CV in cross validation and Grid search same as train test split \n",
        "\n",
        "# n_split is number of split and test_size is how much % of rows you want in test data set\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def do_shufflesplit(n_splits,test_size):  \n",
        "  return ShuffleSplit(n_splits = n_splits, test_size = test_size/100, train_size = 1-(test_size/100), random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1gF0AeY9JkA",
        "colab_type": "text"
      },
      "source": [
        "### 2. Cross Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0UAk5LimY4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross validation with SKlearn\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "def do_crossValidation(algo_object,X,y,cv,scoring):\n",
        "  '''\n",
        "      Return : return data frame \n",
        "  '''\n",
        "  algo_name = algo_object.__class__.__name__ \n",
        "  cc = cross_validate(algo_object, X, y, cv=cv, return_train_score=True, return_estimator=True, n_jobs=-1, scoring=scroing)\n",
        "  return get_crossValidation_Result(algo_name,cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Poo1wzo9_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_crossValidation_Result(algo_name,result):\n",
        "  result_dict = {}\n",
        "  result_dict['Algo Name'] = algo_name\n",
        "  result_dict['Time'] = result['fit_time'].mean()\n",
        "  result_dict['Algo Train Accuracy Score']= result['train_score'].mean()\n",
        "  result_dict['Algo Test Accuracy Score'] = result['test_score'].mean()\n",
        "  result_dict['Algo Test Accuracy 3*STD'] = result['test_score'].std()*3\n",
        "  return pd.DataFrame(result_dict) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQKvm5bv9Jnf",
        "colab_type": "text"
      },
      "source": [
        "### 3. Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n0GaROMgbZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lavi Code for GridSearch\n",
        "\n",
        "# Helper Class for Initilizing GridSearch\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "class EstimatorSelectionHelper:\n",
        "\n",
        "    # Init function as we create object of this class this function call\n",
        "    def __init__(self, models, params):\n",
        "      '''\n",
        "      models and params are dict\n",
        "      '''\n",
        "      if not set(models.keys()).issubset(set(params.keys())):\n",
        "        missing_params = list(set(models.keys()) - set(params.keys()))\n",
        "        raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
        "      self.models = models\n",
        "      self.params = params\n",
        "      self.keys = models.keys()\n",
        "      self.grid_searches = {}\n",
        "      self.best_params = {}\n",
        "\n",
        "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=True):\n",
        "        for key in self.keys:\n",
        "            print(\"Running GridSearchCV for %s.\" % key)\n",
        "            model = self.models[key]\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, refit=refit,\n",
        "                              return_train_score=True)\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs  \n",
        "            self.best_params[key]  = str(gs.best_params_)\n",
        "\n",
        "            # print (gs.best_params_.feature_importances_ )\n",
        "            # try:\n",
        "            #   print(gs.best_params_.feature_importances_ )\n",
        "            #   self.feature_importance[key]= gs.best_params_.feature_importances_ \n",
        "            # except AttributeError:\n",
        "            #   pass\n",
        "\n",
        "    # def Feature_Importance(self):\n",
        "    #   for each\n",
        "\n",
        "    # def returnFeatureImportance(self):\n",
        "\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            print(k)\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params,all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "\n",
        "        return df[columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr3v7Tjagbkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmkKBl429Jrc",
        "colab_type": "text"
      },
      "source": [
        "### 4. Model *Libs*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34RBvLz9JxZ",
        "colab_type": "text"
      },
      "source": [
        "### 5. List of Model and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiJauOqDQITm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of models\n",
        "\n",
        "ML_classification_Algo = [\n",
        "    # Ensemble Learning   \n",
        "    ensemble.RandomForestClassifier(),    \n",
        "    #GLM\n",
        "    linear_model.LogisticRegressionCV(),\n",
        "    #Navies Bayes\n",
        "    naive_bayes.BernoulliNB(),\n",
        "    naive_bayes.GaussianNB(),\n",
        "    \n",
        "    #Nearest Neighbor\n",
        "    neighbors.KNeighborsClassifier(),\n",
        "    \n",
        "    #SVM\n",
        "    svm.SVC(probability=True),\n",
        "    \n",
        "    #Trees    \n",
        "    tree.DecisionTreeClassifier(),\n",
        "    tree.ExtraTreeClassifier(),\n",
        "    \n",
        "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
        "    XGBClassifier()    \n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag1b7XzJ-nCF",
        "colab_type": "text"
      },
      "source": [
        "### 6. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5to9C-7-nFC",
        "colab_type": "text"
      },
      "source": [
        "<a id = Section5></a>\n",
        "## 5. Post Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBY-o1ox-nLQ",
        "colab_type": "text"
      },
      "source": [
        "### 1. Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWM__0yD-nPT",
        "colab_type": "text"
      },
      "source": [
        "### 2. Confusion Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QjjIqTn-nSx",
        "colab_type": "text"
      },
      "source": [
        "### 3. Precession, Recall, F1, AUC ROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcpQ_wS7-nV_",
        "colab_type": "text"
      },
      "source": [
        "### 4. RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJV3xs8AEkZ_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jnzojqeEke6",
        "colab_type": "text"
      },
      "source": [
        "## 6. ML Interpretation | Explaninable AI\n",
        "\n",
        "LIME and SHAP lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtt-0T6MAMZh",
        "colab_type": "text"
      },
      "source": [
        "## 7. Model Deployment | MLOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPl3IGEBAMeM",
        "colab_type": "text"
      },
      "source": [
        "## 8. Create Dashbord"
      ]
    }
  ]
}